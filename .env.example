# Project Directories
ROOT_DIR=d:/llm/fine-tune/qlora_pipeline
DATA_DIR=data
PROCESSED_DATA_DIR=data/processed
MODEL_OUTPUT_DIR=models
PREDICTIONS_DIR=predictions
EVALUATION_DIR=evaluation
LOGS_DIR=logs

# Base Model Configuration
BASE_MODEL_NAME=Qwen/Qwen2.5-0.5B-Instruct
MODEL_REVISION=main
MODEL_CACHE_DIR=models/cache
MODEL_MAX_LENGTH=2048

# Dataset Configuration
DATASET_NAME=physics_qa.json
TRAIN_FILE=data/physics_qa.json
VAL_SPLIT=0.1
TEST_SPLIT=0.1
MAX_SAMPLES=1000
INPUT_COLUMN=input
TARGET_COLUMN=output
MAX_LENGTH=2048
MAX_TARGET_LENGTH=512
MODEL_GEN=gpt-4o

# Training Parameters
PER_DEVICE_TRAIN_BATCH_SIZE=4
EVAL_BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
NUM_TRAIN_EPOCHS=3
MAX_STEPS=-1
LEARNING_RATE=2e-4
WEIGHT_DECAY=0.001
WARMUP_RATIO=0.03
WARMUP_STEPS=0
MAX_GRAD_NORM=1.0
LR_SCHEDULER=cosine
OPTIM=adamw_bnb_8bit
SAVE_STEPS=50
LOGGING_STEPS=10
EVALUATION_STRATEGY=no
EVAL_STEPS=100
SAVE_TOTAL_LIMIT=3
SEED=42

# Mixed Precision Training
FP16=false
BF16=false

# LoRA Configuration
LORA_R=8
LORA_ALPHA=32
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=["q_proj","k_proj","v_proj","o_proj","gate_proj","up_proj","down_proj"]
MODULES_TO_SAVE=[]
FAN_IN_FAN_OUT=false
BIAS=none
USE_GRADIENT_CHECKPOINTING=true

# Quantization Configuration
LOAD_IN_8BIT=false
LOAD_IN_4BIT=true
BITS=4
GROUP_SIZE=128
DOUBLE_QUANT=true
QUANT_TYPE=nf4
USE_NESTED_QUANT=false

# Model Saving and Loading
SAVE_SAFETENSORS=true
RESUME_FROM_CHECKPOINT=
PUSH_TO_HUB=false
HUB_MODEL_ID=
HUB_PRIVATE_REPO=true
HUB_TOKEN=

# OpenAI API Configuration (for evaluation)
OPENAI_API_KEY=your_api_key_here
OPENAI_API_BASE=https://api.openai.com/v1
EVAL_MODEL=gpt-4o
EVAL_TEMPERATURE=0.0
EVAL_MAX_TOKENS=1000
EVAL_BATCH_SIZE=1

# Inference Configuration
PREDICT_BATCH_SIZE=4
MAX_NEW_TOKENS=128
TOP_K=50
TOP_P=0.95
TEMPERATURE=0.7
DO_SAMPLE=true
NUM_RETURN_SEQUENCES=1

# Hardware Configuration
CUDA_VISIBLE_DEVICES=0

# Prompt Template Configuration
PROMPT_TEMPLATE_TYPE=qwen
REGISTER_ADAPTER=true
ADAPTER_NAME=
ADAPTER_DESCRIPTION=
ADAPTER_CONFIG_PATH=config/adapters.json
