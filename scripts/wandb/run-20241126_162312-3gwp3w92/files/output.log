INFO: ==================================================
2024-11-26 16:23:13,367 - INFO - ==================================================
INFO: Configuration:
2024-11-26 16:23:13,368 - INFO - Configuration:
INFO:
Model:
2024-11-26 16:23:13,369 - INFO -
Model:
INFO:   name: Qwen/Qwen2.5-0.5B-Instruct
2024-11-26 16:23:13,369 - INFO -   name: Qwen/Qwen2.5-0.5B-Instruct
INFO:   bits: 4
2024-11-26 16:23:13,370 - INFO -   bits: 4
INFO:   double_quant: True
2024-11-26 16:23:13,371 - INFO -   double_quant: True
INFO:   quant_type: nf4
2024-11-26 16:23:13,371 - INFO -   quant_type: nf4
INFO:
Dataset:
2024-11-26 16:23:13,372 - INFO -
Dataset:
INFO:   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
2024-11-26 16:23:13,372 - INFO -   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
INFO:   name: None
2024-11-26 16:23:13,373 - INFO -   name: None
INFO:   max_samples: 1000
2024-11-26 16:23:13,373 - INFO -   max_samples: 1000
INFO:   max_length: 2048
2024-11-26 16:23:13,374 - INFO -   max_length: 2048
INFO:
Training:
2024-11-26 16:23:13,374 - INFO -
Training:
INFO:   num_epochs: 300
2024-11-26 16:23:13,375 - INFO -   num_epochs: 300
INFO:   batch_size: 4
2024-11-26 16:23:13,375 - INFO -   batch_size: 4
INFO:   learning_rate: 0.0002
2024-11-26 16:23:13,376 - INFO -   learning_rate: 0.0002
INFO:   weight_decay: 0.001
2024-11-26 16:23:13,377 - INFO -   weight_decay: 0.001
INFO:   warmup_ratio: 0.03
2024-11-26 16:23:13,377 - INFO -   warmup_ratio: 0.03
INFO:   lr_scheduler: cosine
2024-11-26 16:23:13,378 - INFO -   lr_scheduler: cosine
INFO:
LoRA:
2024-11-26 16:23:13,378 - INFO -
LoRA:
INFO:   r: 64
2024-11-26 16:23:13,379 - INFO -   r: 64
INFO:   alpha: 128
2024-11-26 16:23:13,380 - INFO -   alpha: 128
INFO:   dropout: 0.05
2024-11-26 16:23:13,381 - INFO -   dropout: 0.05
INFO:
Wandb:
2024-11-26 16:23:13,381 - INFO -
Wandb:
INFO:   project: qlora-finetune
2024-11-26 16:23:13,382 - INFO -   project: qlora-finetune
INFO:   run_name: Qwen2.5-0.5B-Instruct-20241126_162311
2024-11-26 16:23:13,383 - INFO -   run_name: Qwen2.5-0.5B-Instruct-20241126_162311
INFO:   watch: gradients
2024-11-26 16:23:13,383 - INFO -   watch: gradients
INFO:   log_model: checkpoint
2024-11-26 16:23:13,383 - INFO -   log_model: checkpoint
INFO: Loading tokenizer...
2024-11-26 16:23:13,384 - INFO - Loading tokenizer...
INFO: Loading model...
2024-11-26 16:23:14,335 - INFO - Loading model...
Unused kwargs: ['bnb_4bit_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-26 16:23:14,981 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:
==================================================
2024-11-26 16:23:16,427 - INFO -
==================================================
INFO: Model Information:
2024-11-26 16:23:16,437 - INFO - Model Information:
INFO:   Total parameters: 315119488
2024-11-26 16:23:16,438 - INFO -   Total parameters: 315119488
INFO:   Trainable parameters: 136178560
2024-11-26 16:23:16,440 - INFO -   Trainable parameters: 136178560
INFO:   Percentage of trainable parameters: 43.21489631260127
2024-11-26 16:23:16,440 - INFO -   Percentage of trainable parameters: 43.21489631260127
INFO: Preparing model for k-bit training...
2024-11-26 16:23:16,443 - INFO - Preparing model for k-bit training...
INFO: Loading dataset...
2024-11-26 16:23:16,468 - INFO - Loading dataset...
2024-11-26 16:23:16,477 - INFO - Loaded 20 examples from D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
INFO: Dataset loaded with 20 examples
2024-11-26 16:23:16,480 - INFO - Dataset loaded with 20 examples
INFO:
==================================================
2024-11-26 16:23:16,481 - INFO -
==================================================
INFO: Dataset Information:
2024-11-26 16:23:16,483 - INFO - Dataset Information:
INFO: Number of examples: 20
2024-11-26 16:23:16,483 - INFO - Number of examples: 20
INFO: Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
2024-11-26 16:23:16,485 - INFO - Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
INFO: Column names: ['input', 'output']
2024-11-26 16:23:16,485 - INFO - Column names: ['input', 'output']
INFO: First example:
2024-11-26 16:23:16,486 - INFO - First example:
INFO:   input: Question: What is Newton's first law of motion?...
2024-11-26 16:23:16,487 - INFO -   input: Question: What is Newton's first law of motion?...
INFO:   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
2024-11-26 16:23:16,488 - INFO -   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
INFO: Processing dataset...
2024-11-26 16:23:16,489 - INFO - Processing dataset...
Processing dataset: 100%|████████████| 20/20 [00:00<00:00, 2230.89 examples/s]
INFO: Configuring LoRA...
2024-11-26 16:23:16,580 - INFO - Configuring LoRA...
INFO: Applying LoRA...
2024-11-26 16:23:16,580 - INFO - Applying LoRA...
INFO: Configuring training arguments...
2024-11-26 16:23:16,700 - INFO - Configuring training arguments...
INFO: Initializing trainer...
2024-11-26 16:23:16,745 - INFO - Initializing trainer...
D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py:517: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
ERROR: Error: Training failed
2024-11-26 16:23:16,756 - ERROR - Error: Training failed
ERROR: Exception details:
2024-11-26 16:23:16,758 - ERROR - Exception details:
ERROR: ValueError: 'True' is not a valid WandbLogModel

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py", line 517, in train
    trainer = Trainer(
              ^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\utils\deprecation.py", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 630, in __init__
    self.callback_handler = CallbackHandler(
                            ^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer_callback.py", line 411, in __init__
    self.add_callback(cb)
  File "D:\anaconda3\Lib\site-packages\transformers\trainer_callback.py", line 428, in add_callback
    cb = callback() if isinstance(callback, type) else callback
         ^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\integrations\integration_utils.py", line 774, in __init__
    self._log_model = WandbLogModel(os.getenv("WANDB_LOG_MODEL", "false"))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\enum.py", line 757, in __call__
    return cls.__new__(cls, value)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\enum.py", line 1179, in __new__
    raise exc
  File "D:\anaconda3\Lib\enum.py", line 1156, in __new__
    result = cls._missing_(value)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\integrations\integration_utils.py", line 748, in _missing_
    raise DeprecationWarning(
DeprecationWarning: Setting `WANDB_LOG_MODEL` as True is deprecated and will be removed in version 5 of transformers. Use one of `'end'` or `'checkpoint'` instead.
2024-11-26 16:23:16,762 - ERROR - ValueError: 'True' is not a valid WandbLogModel

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py", line 517, in train
    trainer = Trainer(
              ^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\utils\deprecation.py", line 165, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 630, in __init__
    self.callback_handler = CallbackHandler(
                            ^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer_callback.py", line 411, in __init__
    self.add_callback(cb)
  File "D:\anaconda3\Lib\site-packages\transformers\trainer_callback.py", line 428, in add_callback
    cb = callback() if isinstance(callback, type) else callback
         ^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\integrations\integration_utils.py", line 774, in __init__
    self._log_model = WandbLogModel(os.getenv("WANDB_LOG_MODEL", "false"))
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\enum.py", line 757, in __call__
    return cls.__new__(cls, value)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\enum.py", line 1179, in __new__
    raise exc
  File "D:\anaconda3\Lib\enum.py", line 1156, in __new__
    result = cls._missing_(value)
             ^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\integrations\integration_utils.py", line 748, in _missing_
    raise DeprecationWarning(
DeprecationWarning: Setting `WANDB_LOG_MODEL` as True is deprecated and will be removed in version 5 of transformers. Use one of `'end'` or `'checkpoint'` instead.
