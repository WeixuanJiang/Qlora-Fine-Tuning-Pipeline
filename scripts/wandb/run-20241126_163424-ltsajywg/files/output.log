INFO: ==================================================
2024-11-26 16:34:24,971 - INFO - ==================================================
INFO: Configuration:
2024-11-26 16:34:24,976 - INFO - Configuration:
INFO:
Model:
2024-11-26 16:34:24,976 - INFO -
Model:
INFO:   name: Qwen/Qwen2.5-0.5B-Instruct
2024-11-26 16:34:24,977 - INFO -   name: Qwen/Qwen2.5-0.5B-Instruct
INFO:   bits: 4
2024-11-26 16:34:24,978 - INFO -   bits: 4
INFO:   double_quant: True
2024-11-26 16:34:24,979 - INFO -   double_quant: True
INFO:   quant_type: nf4
2024-11-26 16:34:24,980 - INFO -   quant_type: nf4
INFO:
Dataset:
2024-11-26 16:34:24,980 - INFO -
Dataset:
INFO:   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
2024-11-26 16:34:24,981 - INFO -   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
INFO:   name: None
2024-11-26 16:34:24,983 - INFO -   name: None
INFO:   max_samples: 1000
2024-11-26 16:34:24,983 - INFO -   max_samples: 1000
INFO:   max_length: 2048
2024-11-26 16:34:24,984 - INFO -   max_length: 2048
INFO:
Training:
2024-11-26 16:34:24,985 - INFO -
Training:
INFO:   num_epochs: 300
2024-11-26 16:34:24,986 - INFO -   num_epochs: 300
INFO:   batch_size: 4
2024-11-26 16:34:24,986 - INFO -   batch_size: 4
INFO:   learning_rate: 0.0002
2024-11-26 16:34:24,987 - INFO -   learning_rate: 0.0002
INFO:   weight_decay: 0.001
2024-11-26 16:34:24,987 - INFO -   weight_decay: 0.001
INFO:   warmup_ratio: 0.03
2024-11-26 16:34:24,988 - INFO -   warmup_ratio: 0.03
INFO:   lr_scheduler: cosine
2024-11-26 16:34:24,988 - INFO -   lr_scheduler: cosine
INFO:
LoRA:
2024-11-26 16:34:24,989 - INFO -
LoRA:
INFO:   r: 64
2024-11-26 16:34:24,990 - INFO -   r: 64
INFO:   alpha: 128
2024-11-26 16:34:24,990 - INFO -   alpha: 128
INFO:   dropout: 0.05
2024-11-26 16:34:24,991 - INFO -   dropout: 0.05
INFO:
Wandb:
2024-11-26 16:34:24,992 - INFO -
Wandb:
INFO:   project: qlora-finetune
2024-11-26 16:34:24,992 - INFO -   project: qlora-finetune
INFO:   run_name: Qwen2.5-0.5B-Instruct-20241126_163423
2024-11-26 16:34:24,993 - INFO -   run_name: Qwen2.5-0.5B-Instruct-20241126_163423
INFO:   watch: gradients
2024-11-26 16:34:24,995 - INFO -   watch: gradients
INFO:   log_model: checkpoint
2024-11-26 16:34:24,995 - INFO -   log_model: checkpoint
INFO: Loading tokenizer...
2024-11-26 16:34:24,996 - INFO - Loading tokenizer...
INFO: Loading model...
2024-11-26 16:34:25,931 - INFO - Loading model...
Unused kwargs: ['bnb_4bit_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-26 16:34:26,448 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:
==================================================
2024-11-26 16:34:27,476 - INFO -
==================================================
INFO: Model Information:
2024-11-26 16:34:27,481 - INFO - Model Information:
INFO:   Total parameters: 315119488
2024-11-26 16:34:27,483 - INFO -   Total parameters: 315119488
INFO:   Trainable parameters: 136178560
2024-11-26 16:34:27,484 - INFO -   Trainable parameters: 136178560
INFO:   Percentage of trainable parameters: 43.21489631260127
2024-11-26 16:34:27,485 - INFO -   Percentage of trainable parameters: 43.21489631260127
INFO: Preparing model for k-bit training...
2024-11-26 16:34:27,489 - INFO - Preparing model for k-bit training...
INFO: Loading dataset...
2024-11-26 16:34:27,518 - INFO - Loading dataset...
2024-11-26 16:34:27,526 - INFO - Loaded 20 examples from D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
Map: 100%|███████████████████████████| 20/20 [00:00<00:00, 7222.22 examples/s]
INFO: Dataset loaded with 20 examples
2024-11-26 16:34:27,535 - INFO - Dataset loaded with 20 examples
INFO:
==================================================
2024-11-26 16:34:27,536 - INFO -
==================================================
INFO: Dataset Information:
2024-11-26 16:34:27,537 - INFO - Dataset Information:
INFO: Number of examples: 20
2024-11-26 16:34:27,538 - INFO - Number of examples: 20
INFO: Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
2024-11-26 16:34:27,538 - INFO - Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
INFO: Column names: ['input', 'output']
2024-11-26 16:34:27,539 - INFO - Column names: ['input', 'output']
INFO: First example:
2024-11-26 16:34:27,540 - INFO - First example:
INFO:   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
2024-11-26 16:34:27,541 - INFO -   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
INFO:   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
2024-11-26 16:34:27,542 - INFO -   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
INFO: Processing dataset...
2024-11-26 16:34:27,543 - INFO - Processing dataset...
Processing dataset: 100%|████████████| 20/20 [00:00<00:00, 3258.98 examples/s]
INFO: Configuring LoRA...
2024-11-26 16:34:27,629 - INFO - Configuring LoRA...
INFO: Applying LoRA...
2024-11-26 16:34:27,630 - INFO - Applying LoRA...
INFO: Configuring training arguments...
2024-11-26 16:34:27,737 - INFO - Configuring training arguments...
INFO: Initializing trainer...
2024-11-26 16:34:27,782 - INFO - Initializing trainer...
D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py:524: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
INFO: Starting training...
2024-11-26 16:34:27,788 - INFO - Starting training...
  9%|███▌                                    | 27/300 [00:36<06:02,  1.33s/it]Traceback (most recent call last):
{'loss': 4.4091, 'grad_norm': 8.34113597869873, 'learning_rate': 0.00019999417253661235, 'epoch': 8.0}
{'loss': 0.8374, 'grad_norm': 4.872353553771973, 'learning_rate': 0.00019929569837240564, 'epoch': 16.0}
  File "D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py", line 564, in <module>
    fire.Fire(train)
  File "D:\anaconda3\Lib\site-packages\fire\core.py", line 135, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\fire\core.py", line 468, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
                                ^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\fire\core.py", line 684, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^
  File "D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py", line 534, in train
    trainer.train()
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 2123, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 2481, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 3579, in training_step
    loss = self.compute_loss(model, inputs, num_items_in_batch=num_items_in_batch)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\trainer.py", line 3633, in compute_loss
    outputs = model(**inputs)
              ^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\peft\peft_model.py", line 1430, in forward
    return self.base_model(
           ^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\peft\tuners\tuners_utils.py", line 179, in forward
    return self.model.forward(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\accelerate\hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 1164, in forward
    outputs = self.model(
              ^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\accelerate\hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 883, in forward
    layer_outputs = self._gradient_checkpointing_func(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\_dynamo\eval_frame.py", line 632, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\utils\checkpoint.py", line 496, in checkpoint
    ret = function(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\accelerate\hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 623, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
                                                          ^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\accelerate\hooks.py", line 170, in new_forward
    output = module._old_forward(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\transformers\models\qwen2\modeling_qwen2.py", line 501, in forward
    query_states = self.q_proj(hidden_states)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\peft\tuners\lora\bnb.py", line 474, in forward
    output = lora_B(lora_A(dropout(x))) * scaling
                    ^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "D:\anaconda3\Lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
