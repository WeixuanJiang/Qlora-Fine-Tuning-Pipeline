INFO: ==================================================
2024-11-26 16:45:03,542 - INFO - ==================================================
INFO: Configuration:
2024-11-26 16:45:03,545 - INFO - Configuration:
INFO:
Model:
2024-11-26 16:45:03,545 - INFO -
Model:
INFO:   name: Qwen/Qwen2.5-0.5B-Instruct
2024-11-26 16:45:03,546 - INFO -   name: Qwen/Qwen2.5-0.5B-Instruct
INFO:   bits: 4
2024-11-26 16:45:03,547 - INFO -   bits: 4
INFO:   double_quant: True
2024-11-26 16:45:03,548 - INFO -   double_quant: True
INFO:   quant_type: nf4
2024-11-26 16:45:03,548 - INFO -   quant_type: nf4
INFO:
Dataset:
2024-11-26 16:45:03,548 - INFO -
Dataset:
INFO:   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
2024-11-26 16:45:03,549 - INFO -   path: D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
INFO:   name: None
2024-11-26 16:45:03,551 - INFO -   name: None
INFO:   max_samples: 1000
2024-11-26 16:45:03,552 - INFO -   max_samples: 1000
INFO:   max_length: 2048
2024-11-26 16:45:03,553 - INFO -   max_length: 2048
INFO:
Training:
2024-11-26 16:45:03,554 - INFO -
Training:
INFO:   num_epochs: 50
2024-11-26 16:45:03,556 - INFO -   num_epochs: 50
INFO:   batch_size: 4
2024-11-26 16:45:03,557 - INFO -   batch_size: 4
INFO:   learning_rate: 0.0002
2024-11-26 16:45:03,557 - INFO -   learning_rate: 0.0002
INFO:   weight_decay: 0.001
2024-11-26 16:45:03,558 - INFO -   weight_decay: 0.001
INFO:   warmup_ratio: 0.03
2024-11-26 16:45:03,558 - INFO -   warmup_ratio: 0.03
INFO:   lr_scheduler: cosine
2024-11-26 16:45:03,559 - INFO -   lr_scheduler: cosine
INFO:
LoRA:
2024-11-26 16:45:03,559 - INFO -
LoRA:
INFO:   r: 64
2024-11-26 16:45:03,560 - INFO -   r: 64
INFO:   alpha: 128
2024-11-26 16:45:03,561 - INFO -   alpha: 128
INFO:   dropout: 0.05
2024-11-26 16:45:03,562 - INFO -   dropout: 0.05
INFO:
Wandb:
2024-11-26 16:45:03,563 - INFO -
Wandb:
INFO:   project: qlora-finetune
2024-11-26 16:45:03,564 - INFO -   project: qlora-finetune
INFO:   run_name: Qwen2.5-0.5B-Instruct-20241126_164501
2024-11-26 16:45:03,565 - INFO -   run_name: Qwen2.5-0.5B-Instruct-20241126_164501
INFO:   watch: gradients
2024-11-26 16:45:03,567 - INFO -   watch: gradients
INFO:   log_model: checkpoint
2024-11-26 16:45:03,568 - INFO -   log_model: checkpoint
INFO: Loading tokenizer...
2024-11-26 16:45:03,568 - INFO - Loading tokenizer...
INFO: Loading model...
2024-11-26 16:45:04,191 - INFO - Loading model...
Unused kwargs: ['bnb_4bit_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-26 16:45:05,296 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:
==================================================
2024-11-26 16:45:06,778 - INFO -
==================================================
INFO: Model Information:
2024-11-26 16:45:06,785 - INFO - Model Information:
INFO:   Total parameters: 315119488
2024-11-26 16:45:06,787 - INFO -   Total parameters: 315119488
INFO:   Trainable parameters: 136178560
2024-11-26 16:45:06,788 - INFO -   Trainable parameters: 136178560
INFO:   Percentage of trainable parameters: 43.21489631260127
2024-11-26 16:45:06,789 - INFO -   Percentage of trainable parameters: 43.21489631260127
INFO: Preparing model for k-bit training...
2024-11-26 16:45:06,792 - INFO - Preparing model for k-bit training...
INFO: Loading dataset...
2024-11-26 16:45:06,811 - INFO - Loading dataset...
2024-11-26 16:45:06,823 - INFO - Loaded 20 examples from D:/llm/fine-tune/qlora_pipeline\data\physics_test_qa.json
Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 5908.72 examples/s]
INFO: Dataset loaded with 20 examples
2024-11-26 16:45:06,834 - INFO - Dataset loaded with 20 examples
INFO:
==================================================
2024-11-26 16:45:06,836 - INFO -
==================================================
INFO: Dataset Information:
2024-11-26 16:45:06,836 - INFO - Dataset Information:
INFO: Number of examples: 20
2024-11-26 16:45:06,837 - INFO - Number of examples: 20
INFO: Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
2024-11-26 16:45:06,838 - INFO - Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
INFO: Column names: ['input', 'output']
2024-11-26 16:45:06,839 - INFO - Column names: ['input', 'output']
INFO: First example:
2024-11-26 16:45:06,839 - INFO - First example:
INFO:   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
2024-11-26 16:45:06,840 - INFO -   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
INFO:   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
2024-11-26 16:45:06,841 - INFO -   output: Answer: Newton's first law of motion states that an object will remain at rest or in uniform motion ...
INFO: Processing dataset...
2024-11-26 16:45:06,842 - INFO - Processing dataset...
Processing dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [00:00<00:00, 1187.97 examples/s]
INFO: Configuring LoRA...
2024-11-26 16:45:06,925 - INFO - Configuring LoRA...
INFO: Applying LoRA...
2024-11-26 16:45:06,926 - INFO - Applying LoRA...
INFO: Configuring training arguments...
2024-11-26 16:45:07,045 - INFO - Configuring training arguments...
INFO: Initializing trainer...
2024-11-26 16:45:07,097 - INFO - Initializing trainer...
D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py:524: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
INFO: Starting training...
2024-11-26 16:45:07,110 - INFO - Starting training...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:08<00:00,  1.45s/it][34m[1mwandb[0m: Adding directory to artifact (D:\llm\fine-tune\qlora_pipeline\model_output\run_2024-26-11_0444\checkpoint-50)... Done. 0.2s
{'loss': 3.3175, 'grad_norm': 7.496428966522217, 'learning_rate': 0.00018660254037844388, 'epoch': 8.0}
{'loss': 0.4747, 'grad_norm': 1.7909761667251587, 'learning_rate': 0.000138268343236509, 'epoch': 16.0}
{'loss': 0.1052, 'grad_norm': 0.6117176413536072, 'learning_rate': 7.411809548974792e-05, 'epoch': 24.0}
{'loss': 0.072, 'grad_norm': 0.774660050868988, 'learning_rate': 2.0664665970876496e-05, 'epoch': 32.0}
{'loss': 0.0654, 'grad_norm': 0.7627013921737671, 'learning_rate': 0.0, 'epoch': 40.0}
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [01:18<00:00,  1.57s/it]
{'train_runtime': 73.2858, 'train_samples_per_second': 13.645, 'train_steps_per_second': 0.682, 'train_loss': 0.806967830657959, 'epoch': 40.0}
INFO: Saving model...
2024-11-26 16:46:26,007 - INFO - Saving model...
