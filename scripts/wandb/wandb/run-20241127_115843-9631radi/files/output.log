INFO: ==================================================
2024-11-27 11:58:44,041 - INFO - ==================================================
INFO: Configuration:
2024-11-27 11:58:44,043 - INFO - Configuration:
INFO:
Model:
2024-11-27 11:58:44,043 - INFO -
Model:
INFO:   name: Qwen/Qwen2.5-0.5B-Instruct
2024-11-27 11:58:44,044 - INFO -   name: Qwen/Qwen2.5-0.5B-Instruct
INFO:   bits: 4
2024-11-27 11:58:44,045 - INFO -   bits: 4
INFO:   double_quant: true
2024-11-27 11:58:44,046 - INFO -   double_quant: true
INFO:   quant_type: nf4
2024-11-27 11:58:44,046 - INFO -   quant_type: nf4
INFO:
Dataset:
2024-11-27 11:58:44,046 - INFO -
Dataset:
INFO:   path: d:/llm/fine-tune/qlora_pipeline\data\physics_qa.json
2024-11-27 11:58:44,048 - INFO -   path: d:/llm/fine-tune/qlora_pipeline\data\physics_qa.json
INFO:   name: None
2024-11-27 11:58:44,048 - INFO -   name: None
INFO:   max_samples: 1000
2024-11-27 11:58:44,049 - INFO -   max_samples: 1000
INFO:   max_length: 2048
2024-11-27 11:58:44,051 - INFO -   max_length: 2048
INFO:
Training:
2024-11-27 11:58:44,052 - INFO -
Training:
INFO:   num_epochs: 3
2024-11-27 11:58:44,053 - INFO -   num_epochs: 3
INFO:   batch_size: 4
2024-11-27 11:58:44,053 - INFO -   batch_size: 4
INFO:   learning_rate: 0.0002
2024-11-27 11:58:44,055 - INFO -   learning_rate: 0.0002
INFO:   weight_decay: 0.001
2024-11-27 11:58:44,056 - INFO -   weight_decay: 0.001
INFO:   warmup_ratio: 0.03
2024-11-27 11:58:44,056 - INFO -   warmup_ratio: 0.03
INFO:   lr_scheduler: cosine
2024-11-27 11:58:44,056 - INFO -   lr_scheduler: cosine
INFO:
LoRA:
2024-11-27 11:58:44,057 - INFO -
LoRA:
INFO:   r: 8
2024-11-27 11:58:44,061 - INFO -   r: 8
INFO:   alpha: 32
2024-11-27 11:58:44,061 - INFO -   alpha: 32
INFO:   dropout: 0.1
2024-11-27 11:58:44,062 - INFO -   dropout: 0.1
INFO:
Wandb:
2024-11-27 11:58:44,063 - INFO -
Wandb:
INFO:   project: qwen-qlora
2024-11-27 11:58:44,064 - INFO -   project: qwen-qlora
INFO:   run_name: Qwen2.5-0.5B-Instruct-20241127_115842
2024-11-27 11:58:44,064 - INFO -   run_name: Qwen2.5-0.5B-Instruct-20241127_115842
INFO:   watch: gradients
2024-11-27 11:58:44,066 - INFO -   watch: gradients
INFO:   log_model: checkpoint
2024-11-27 11:58:44,074 - INFO -   log_model: checkpoint
INFO: Loading tokenizer...
2024-11-27 11:58:44,078 - INFO - Loading tokenizer...
INFO: Loading model...
2024-11-27 11:58:44,670 - INFO - Loading model...
Unused kwargs: ['bnb_4bit_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.
2024-11-27 11:58:46,100 - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
INFO:
==================================================
2024-11-27 11:58:47,574 - INFO -
==================================================
INFO: Model Information:
2024-11-27 11:58:47,576 - INFO - Model Information:
INFO:   Total parameters: 315119488
2024-11-27 11:58:47,577 - INFO -   Total parameters: 315119488
INFO:   Trainable parameters: 136178560
2024-11-27 11:58:47,577 - INFO -   Trainable parameters: 136178560
INFO:   Percentage of trainable parameters: 43.21489631260127
2024-11-27 11:58:47,578 - INFO -   Percentage of trainable parameters: 43.21489631260127
INFO: Preparing model for k-bit training...
2024-11-27 11:58:47,579 - INFO - Preparing model for k-bit training...
INFO: Loading dataset...
2024-11-27 11:58:47,602 - INFO - Loading dataset...
2024-11-27 11:58:47,621 - INFO - Loaded 20 examples from d:/llm/fine-tune/qlora_pipeline\data\physics_qa.json
Map: 100%|██████████████████████████████████████████████████████████████████████| 20/20 [00:00<00:00, 2315.18 examples/s]
INFO: Dataset loaded with 20 examples
2024-11-27 11:58:47,634 - INFO - Dataset loaded with 20 examples
INFO:
==================================================
2024-11-27 11:58:47,636 - INFO -
==================================================
INFO: Dataset Information:
2024-11-27 11:58:47,637 - INFO - Dataset Information:
INFO: Number of examples: 20
2024-11-27 11:58:47,637 - INFO - Number of examples: 20
INFO: Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
2024-11-27 11:58:47,638 - INFO - Features: {'input': Value(dtype='string', id=None), 'output': Value(dtype='string', id=None)}
INFO: Column names: ['input', 'output']
2024-11-27 11:58:47,641 - INFO - Column names: ['input', 'output']
INFO: First example:
2024-11-27 11:58:47,644 - INFO - First example:
INFO:   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
2024-11-27 11:58:47,645 - INFO -   input: You are a helpful AI assistant that follows instructions carefully.

<|im_start|>user
Question: What...
INFO:   output: Answer: Superposition is a fundamental principle of quantum mechanics that allows a quantum system t...
2024-11-27 11:58:47,646 - INFO -   output: Answer: Superposition is a fundamental principle of quantum mechanics that allows a quantum system t...
INFO: Processing dataset...
2024-11-27 11:58:47,647 - INFO - Processing dataset...
Processing dataset: 100%|███████████████████████████████████████████████████████| 20/20 [00:00<00:00, 1819.10 examples/s]
INFO: Configuring LoRA...
2024-11-27 11:58:47,735 - INFO - Configuring LoRA...
INFO: Applying LoRA...
2024-11-27 11:58:47,736 - INFO - Applying LoRA...
INFO: Configuring training arguments...
2024-11-27 11:58:47,821 - INFO - Configuring training arguments...
INFO: Initializing trainer...
2024-11-27 11:58:47,915 - INFO - Initializing trainer...
D:\llm\fine-tune\qlora_pipeline\scripts\..\train.py:550: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
INFO: Starting training...
2024-11-27 11:58:47,936 - INFO - Starting training...
100%|████████████████████████████████████████████████████████████████████████████████████| 15/15 [00:07<00:00,  2.13it/s]
{'loss': 3.4939, 'grad_norm': 6.035980701446533, 'learning_rate': 5.6611626088244194e-05, 'epoch': 2.0}
{'train_runtime': 7.0508, 'train_samples_per_second': 8.51, 'train_steps_per_second': 2.127, 'train_loss': 2.8583312670389813, 'epoch': 3.0}
INFO: Saving model...
2024-11-27 11:58:55,259 - INFO - Saving model...
